# arXiv Paper Bot Configuration

# ===== arXiv Fetch Settings =====
arxiv:
  # arXiv categories to fetch (e.g., cs.CV, cs.AI, cs.LG, stat.ML)
  categories:
    - cs.CV
    - cs.AI
    - cs.LG

  # Maximum number of papers to fetch per category per day
  max_results: 50

  # Sort order: "submittedDate" (newest first) or "lastUpdatedDate"
  sort_by: submittedDate

  # Sort order: "descending" or "ascending"
  sort_order: descending

  # Fetch mode - determines how papers are retrieved:
  # - "category_only": Fetch by categories only (default)
  # - "keyword_only": Fetch by keywords only (uses search_keywords below)
  # - "combined": Fetch by categories AND keywords (most efficient ⭐ recommended)
  # - "category_then_filter": Fetch all category papers, then filter locally
  fetch_mode: combined

  # 当使用关键词抓取时,是否额外拉取完整分类(用于本地二次过滤)
  fetch_full_categories: false

  # Keywords for arXiv API search (used in keyword_only and combined modes)
  # These keywords are used to query arXiv API directly for efficiency
  search_keywords:
    - transformer
    - diffusion
    - multimodal


# ===== Filter Settings =====
filter:
  # Enable/disable keyword filtering
  enabled: true

  # Filter mode:
  # - "static": Keyword-based only (default, no LLM needed)
  # - "dynamic": With preference learning (placeholder)
  # - "agent": LangGraph agent-enhanced filtering (requires OpenAI API)
  mode: static  # Change to "agent" to enable LangGraph pipeline

  # Keywords for filtering (case-insensitive)
  keywords:
    # High priority keywords (weight: 3.0)
    high_priority:
      - transformer
      - diffusion
      - multimodal
      - vision-language
      - self-supervised

    # Medium priority keywords (weight: 2.0)
    medium_priority:
      - detection
      - segmentation
      - classification
      - generation
      - representation learning

    # Low priority keywords (weight: 1.0)
    low_priority:
      - deep learning
      - neural network
      - machine learning

  # Minimum score threshold to include a paper
  min_score: 1.0

  # Maximum number of papers to keep after filtering
  top_k: 20


# ===== Summarization Settings =====
summarization:
  # Enable/disable AI summarization
  enabled: false

  # Provider: "openai", "glm", "gemini"
  provider: gemini

  # API configuration (use environment variables for API keys)
  api:
    base_url: "https://api.aioec.tech/v1"
    model: "gemini-2.5-flash"
    api_key_env: "ARXIV_API_KEY"  # Read from environment variable

  # Summary fields to generate
  fields:
    - one_sentence_highlight
    - core_method
    - key_contributions


# ===== Storage Settings =====
storage:
  # Output format: "json", "csv", or "both"
  format: both

  # Output paths
  json_path: data/papers.json
  csv_path: data/papers.csv

  # Append to existing files or overwrite
  append_mode: true

  # Include full abstract in output
  include_abstract: true


# ===== Logging Settings =====
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR
  log_file: logs/arxiv_bot.log
  console_output: true


# ===== Schedule Settings =====
schedule:
  # Enable automatic scheduling
  enabled: false

  # Run time (24-hour format, e.g., "09:00" for 9 AM)
  run_time: "09:00"

  # Timezone
  timezone: Asia/Shanghai


# ===== Personalization Settings =====
personalization:
  # Enable personalization features
  enabled: false  # Set to true when using filter.mode: agent

  # User feedback collection
  feedback:
    enabled: true  # Feedback collection is available via feedback.py CLI
    feedback_dir: data/feedback
    auto_save: true

  # Vector similarity ranking (Reserved - not implemented in pure LLM mode)
  vector_ranking:
    enabled: false
    model: allenai/specter
    weight: 0.4
    vector_db: chromadb

  # LangGraph Agent configuration
  agent:
    # Enable agent pipeline (required when filter.mode: agent)
    enabled: false  # Set to true to enable LangGraph pipeline

    # LLM Provider settings
    # Supports: openai, openrouter
    provider: openrouter

    # Model configuration (dual-model support)
    # - reasoning_model: For complex tasks requiring deep thinking (analysis, scoring)
    # - simple_model: For simple tasks (formatting, extraction)
    models:
      reasoning: tngtech/deepseek-r1t2-chimera:free  # 需要深度思考的任务
      simple: z-ai/glm-4.5-air:free                   # 简单任务

    # Legacy single model (for backward compatibility)
    model: z-ai/glm-4.5-air:free

    temperature: 0.3

    # API configuration
    api:
      # For OpenRouter:
      base_url: "https://openrouter.ai/api/v1"
      api_key_env: OPENROUTER_API_KEY
      api_key_file: ""      # 可选：从文件读取 key（避免明文写入配置）
      # For OpenAI (alternative):
      # base_url: ""  # Leave empty for default OpenAI
      # api_key_env: OPENAI_API_KEY
      timeout: 60  # Increased for reasoning model

    # Feature flags
    features:
      preference_analysis: true      # Analyze user reading patterns
      dynamic_query_generation: true # Generate optimized search queries
      result_validation: true        # Validate and re-rank results
      recommendation_explain: true   # Generate recommendation explanations

    # Cold-Start mode (NEW)
    # Allows agent to work without historical feedback
    cold_start:
      enabled: true                  # Enable Cold-Start mode
      use_config_keywords: true      # Use filter keywords as interest profile

    # Flow control
    max_iterations: 2               # Max re-ranking iterations
    confidence_threshold: 0.7       # Below this triggers re-ranking
    # min_feedback_count: 3         # Removed - not needed with Cold-Start

    # Score weights (pure LLM mode)
    weights:
      keyword_score: 0.5           # Weight for keyword matching
      agent_score: 0.5             # Weight for agent adjustments


# ===== Notification Settings =====
notification:
  enabled: false            # 是否启用推送
  provider: feishu          # 可选: feishu / telegram / wechat
  top_k: 5                  # 推送的论文条数

  # 增强选项
  use_rich_format: true     # 使用富文本格式 (飞书卡片/Telegram Markdown)
  include_abstract: false   # 是否包含论文摘要

  feishu:
    webhook_url: ""         # 飞书群机器人 Webhook
    webhook_file: ""        # 或者从文件读取 webhook（避免明文写入配置）
    secret: ""             # 如果开启签名校验,填写密钥

  telegram:
    bot_token: ""          # Telegram Bot Token
    chat_id: ""            # 目标 chat_id

  wechat:
    app_id: ""             # 微信公众号 APP ID
    app_secret: ""        # 微信公众号 APP Secret
    open_id: ""           # 接收者 OpenID (需先关注公众号)


# ===== Publisher Settings (公众号图文发布) =====
publisher:
  enabled: false            # 是否启用公众号发布
  auto_publish: false       # 是否自动发布 (否则只保存到草稿箱)

  # 文章设置
  author: "arXiv Paper Bot" # 文章作者名
  include_abstract: true    # 是否包含摘要
  include_authors: true     # 是否包含论文作者

  wechat:
    app_id: ""              # 微信公众号 APP ID
    app_secret: ""          # 微信公众号 APP Secret
    default_cover: ""       # 默认封面图 media_id (需先上传)


# ===== Daily Topic Pipeline (LangGraph, calendar-day) =====
# Run with: python -m src.pipeline.run_daily --config config/config.yaml --day YYYY-MM-DD
daily:
  timezone: Asia/Shanghai

  # Categories to fetch for the day window (calendar day in timezone)
  categories:
    - cs.AI
    - cs.CV
    - cs.IR
    - cs.HC
    - cs.CL
    - cs.LG

  # Max results per category (increase to reduce truncation risk)
  max_results: 2000

  # Optional rubric text override (defaults to built-in rubric in src/pipeline/topics.py)
  # rubric_text: "..."

  # Drop papers with relevance below this threshold (0-1)
  relevance_threshold: 0.55

  # Use LLM for routing+scoring (falls back to rule-only if key is missing)
  llm_enabled: true

  # LLM call scope: "all" | "ambiguous_only"
  llm_scope: all

  # Batch size for LLM calls
  llm_batch_size: 1

  # Also write raw/enriched/routed/scored JSONL files under data/index/<day>/
  save_intermediates: true
