{"arxiv_id": "2602.06042v1", "title": "Pseudo-Invertible Neural Networks", "abstract": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.", "authors": ["Yamit Ehrlich", "Nimrod Berman", "Assaf Shocher"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.CV"], "pdf_url": "https://arxiv.org/pdf/2602.06042v1", "entry_url": "http://arxiv.org/abs/2602.06042v1", "published": "2026-02-05T18:59:58+00:00", "updated": "2026-02-05T18:59:58+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:20.286840", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "pseudo-invertible neural networks\nthe moore-penrose pseudo-inverse (pinv) serves as the fundamental solution for linear systems. in this paper, we propose a natural generalization of pinv to the nonlinear regime in general and to neural networks in particular. we introduce surjective pseudo-invertible neural networks (spnn), a class of architectures explicitly designed to admit a tractable non-linear pinv. the proposed non-linear pinv and its implementation in spnn satisfy fundamental geometric properties. one such property is null-space projection or \"back-projection\", $x' = x + a^\\dagger(y-ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $ax=y$. we formalize non-linear back-projection (nlbp), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined pinv. we leverage spnns to expand the scope of zero-shot inverse problems. diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. we extend this method to non-linear degradations. here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. this approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.\nnone\ncs.lg\ncs.lg cs.cv", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.06020v1", "title": "Mechanisms of AI Protein Folding in ESMFold", "abstract": "How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.", "authors": ["Kevin Lu", "Jannik Brinkmann", "Stefan Huber", "Aaron Mueller", "Yonatan Belinkov", "David Bau", "Chris Wendler"], "primary_category": "cs.LG", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2602.06020v1", "entry_url": "http://arxiv.org/abs/2602.06020v1", "published": "2026-02-05T18:54:54+00:00", "updated": "2026-02-05T18:54:54+00:00", "comment": "Our code, data, and results are available at https://folding.baulab.info", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.288335", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "mechanisms of ai protein folding in esmfold\nhow do protein structure prediction models fold proteins? we investigate this question by tracing how esmfold folds a beta hairpin, a prevalent structural motif. through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. in the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. in the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. we demonstrate that the mechanisms underlying structural decisions of esmfold can be localized, traced through interpretable representations, and manipulated with strong causal effects.\nour code, data, and results are available at https://folding.baulab.info\ncs.lg\ncs.lg q-bio.bm", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.06014v1", "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference", "abstract": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.", "authors": ["Shunxing Yan", "Han Zhong"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.06014v1", "entry_url": "http://arxiv.org/abs/2602.06014v1", "published": "2026-02-05T18:52:54+00:00", "updated": "2026-02-05T18:52:54+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896053", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "optimism stabilizes thompson sampling for adaptive inference\nthompson sampling (ts) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. we study this phenomenon in the $k$-armed gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. first, we prove that variance-inflated ts \\citep{halder2025stable} is stable for any $k \\ge 2$, including the challenging regime where multiple arms are optimal. this resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $k$-armed setting. second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. in summary, suitably implemented optimism stabilizes thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.\nnone\ncs.lg\ncs.lg cs.ai math.oc math.st stat.ml", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.06000v1", "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods", "abstract": "Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.", "authors": ["Ali Shendabadi", "Parnia Izadirad", "Mostafa Salehi", "Mahmoud Bijankhan"], "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2602.06000v1", "entry_url": "http://arxiv.org/abs/2602.06000v1", "published": "2026-02-05T18:46:28+00:00", "updated": "2026-02-05T18:46:28+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896213", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "speech emotion recognition leveraging openai's whisper representations and attentive pooling methods\nspeech emotion recognition (ser) research has faced limitations due to the lack of standard and sufficiently large datasets. recent studies have leveraged pre-trained models to extract features for downstream tasks such as ser. this work explores the capabilities of whisper, a pre-trained asr system, in speech emotion recognition by proposing two attention-based pooling methods, multi-head attentive average pooling and qkv pooling, designed to efficiently reduce the dimensionality of whisper representations while preserving emotional features. we experiment on english and persian, using the iemocap and shemo datasets respectively, with whisper tiny and small. our multi-head qkv architecture achieves state-of-the-art results on the shemo dataset, with a 2.47% improvement in unweighted accuracy. we further compare the performance of different whisper encoder layers and find that intermediate layers often perform better for ser on the persian dataset, providing a lightweight and efficient alternative to much larger models such as hubert x-large. our findings highlight the potential of whisper as a representation extractor for ser and demonstrate the effectiveness of attention-based pooling for dimension reduction.\nnone\ncs.ai\ncs.ai cs.cl", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05983v1", "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins", "abstract": "The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.", "authors": ["Krešimir Kušić", "Vinny Cahill", "Ivana Dusparic"], "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.05983v1", "entry_url": "http://arxiv.org/abs/2602.05983v1", "published": "2026-02-05T18:33:03+00:00", "updated": "2026-02-05T18:33:03+00:00", "comment": "IEEE IV2026 37th IEEE Intelligent Vehicles Symposium", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896368", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "geographically-aware transformer-based traffic forecasting for urban motorway digital twins\nthe operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. to function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. to improve motorway traffic forecasting, this paper introduces a geographically-aware transformer-based traffic forecasting gattf model, which exploits the geographical relationships between distributed sensors using their mutual information (mi). the model has been evaluated using real-time data from the geneva motorway network in switzerland and results confirm that incorporating geographical awareness through mi enhances the accuracy of gattf forecasting compared to a standard transformer, without increasing model complexity.\nieee iv2026 37th ieee intelligent vehicles symposium\ncs.ai\ncs.ai", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05977v1", "title": "Clifford Kolmogorov-Arnold Networks", "abstract": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.", "authors": ["Matthias Wolff", "Francesco Alesiani", "Christof Duhme", "Xiaoyi Jiang"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.05977v1", "entry_url": "http://arxiv.org/abs/2602.05977v1", "published": "2026-02-05T18:25:40+00:00", "updated": "2026-02-05T18:25:40+00:00", "comment": "This work has been submitted to the IEEE for possible publication", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896410", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "clifford kolmogorov-arnold networks\nwe introduce clifford kolmogorov-arnold network (clkan), a flexible and efficient architecture for function approximation in arbitrary clifford algebra spaces. we propose the use of randomized quasi monte carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. our clkan also introduces new batch normalization strategies to deal with variable domain input. clkan finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.\nthis work has been submitted to the ieee for possible publication\ncs.lg\ncs.lg cs.ai", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05936v1", "title": "Dimensionality Reduction on Riemannian Manifolds in Data Analysis", "abstract": "In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.", "authors": ["Alaa El Ichi", "Khalide Jbilou"], "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.05936v1", "entry_url": "http://arxiv.org/abs/2602.05936v1", "published": "2026-02-05T17:46:58+00:00", "updated": "2026-02-05T17:46:58+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.289147", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "dimensionality reduction on riemannian manifolds in data analysis\nin this work, we investigate riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. in particular, we focus on principal geodesic analysis (pga) as a nonlinear generalization of pca for manifold valued data, and extend discriminant analysis through riemannian adaptations of other known dimensionality reduction methods. these approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. we also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. experimental results on representative datasets demonstrate that riemannian methods provide improved representation quality and classification performance compared to their euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. this study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.\nnone\ncs.lg\ncs.lg", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05898v1", "title": "Universal approximation with signatures of non-geometric rough paths", "abstract": "We establish a universal approximation theorem for signatures of rough paths that are not necessarily weakly geometric. By extending the path with time and its rough path bracket terms, we prove that linear functionals of the signature of the resulting rough paths approximate continuous functionals on rough path spaces uniformly on compact sets. Moreover, we construct the signature of a path extended by its pathwise quadratic variation terms based on general pathwise stochastic integration à la Föllmer, in particular, allowing for pathwise Itô, Stratonovich, and backward Itô integration. In a probabilistic setting, we obtain a universal approximation result for linear functionals of the signature of continuous semimartingales extended by the quadratic variation terms, defined via stochastic Itô integration. Numerical examples illustrate the use of signatures when the path is extended by time and quadratic variation in the context of model calibration and option pricing in mathematical finance.", "authors": ["Mihriban Ceylan", "Anna P. Kwossek", "David J. Prömel"], "primary_category": "math.PR", "categories": ["math.PR", "cs.LG", "q-fin.MF"], "pdf_url": "https://arxiv.org/pdf/2602.05898v1", "entry_url": "http://arxiv.org/abs/2602.05898v1", "published": "2026-02-05T17:16:25+00:00", "updated": "2026-02-05T17:16:25+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.289465", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "universal approximation with signatures of non-geometric rough paths\nwe establish a universal approximation theorem for signatures of rough paths that are not necessarily weakly geometric. by extending the path with time and its rough path bracket terms, we prove that linear functionals of the signature of the resulting rough paths approximate continuous functionals on rough path spaces uniformly on compact sets. moreover, we construct the signature of a path extended by its pathwise quadratic variation terms based on general pathwise stochastic integration à la föllmer, in particular, allowing for pathwise itô, stratonovich, and backward itô integration. in a probabilistic setting, we obtain a universal approximation result for linear functionals of the signature of continuous semimartingales extended by the quadratic variation terms, defined via stochastic itô integration. numerical examples illustrate the use of signatures when the path is extended by time and quadratic variation in the context of model calibration and option pricing in mathematical finance.\nnone\nmath.pr\nmath.pr cs.lg q-fin.mf", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05896v1", "title": "Parity, Sensitivity, and Transformers", "abstract": "The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.\n  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.", "authors": ["Alexander Kozachinskiy", "Tomasz Steifer", "Przemysław Wałȩga"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.05896v1", "entry_url": "http://arxiv.org/abs/2602.05896v1", "published": "2026-02-05T17:14:33+00:00", "updated": "2026-02-05T17:14:33+00:00", "comment": "15 pages", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896814", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "parity, sensitivity, and transformers\nthe transformer architecture is almost a decade old. despite that, we still have a limited understanding of what this architecture can or cannot compute. for instance, can a 1-layer transformer solve parity -- or more generally -- which kinds of transformers can do it? known constructions for parity have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.\n  we give a new construction of a transformer for parity with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. we also give the first lower bound for transformers solving parity -- by showing that it cannot be done with only one layer and one head.\n15 pages\ncs.lg\ncs.lg cs.ai", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05887v1", "title": "Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting", "abstract": "Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.", "authors": ["Tianqi Shen", "Jinji Yang", "Junze He", "Kunhan Gao", "Ziye Ma"], "primary_category": "cs.LG", "categories": ["cs.LG", "math.OC"], "pdf_url": "https://arxiv.org/pdf/2602.05887v1", "entry_url": "http://arxiv.org/abs/2602.05887v1", "published": "2026-02-05T17:05:02+00:00", "updated": "2026-02-05T17:05:02+00:00", "comment": "48 pages, 10 figures, 5 tables. Submitted to Mathematical Programming", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.289679", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "escaping local minima provably in non-convex matrix sensing: a deterministic framework via simulated lifting\nlow-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. motivated by this observation, we propose a simulated oracle direction (sod) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. in essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. to the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. we believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.\n48 pages, 10 figures, 5 tables. submitted to mathematical programming\ncs.lg\ncs.lg math.oc", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05884v1", "title": "Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views", "abstract": "Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\\pm$ 4.26 mL vs. 8.14 $\\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\\pm$ 7.37 mL vs. 37.76 $\\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.", "authors": ["Gino E. Jansen", "Carolina Brás", "R. Nils Planken", "Mark J. Schuuring", "Berto J. Bouma", "Ivana Išgum"], "primary_category": "cs.CV", "categories": ["cs.CV", "cs.AI", "cs.CE"], "pdf_url": "https://arxiv.org/pdf/2602.05884v1", "entry_url": "http://arxiv.org/abs/2602.05884v1", "published": "2026-02-05T17:00:59+00:00", "updated": "2026-02-05T17:00:59+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.896949", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "neural implicit 3d cardiac shape reconstruction from sparse ct angiography slices mimicking 2d transthoracic echocardiography views\naccurate 3d representations of cardiac structures allow quantitative analysis of anatomy and function. in this work, we propose a method for reconstructing complete 3d cardiac shapes from segmentations of sparse planes in ct angiography (cta) for application in 2d transthoracic echocardiography (tte). our method uses a neural implicit function to reconstruct the 3d shape of the cardiac chambers and left-ventricle myocardium from sparse cta planes. to investigate the feasibility of achieving 3d reconstruction from 2d tte, we select planes that mimic the standard apical 2d tte views. during training, a multi-layer perceptron learns shape priors from 3d segmentations of the target structures in cta. at test time, the network reconstructs 3d cardiac shapes from segmentations of tte-mimicking cta planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3d space. for each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference cta volumes. on a held-out set of cta segmentations, our approach achieves an average dice coefficient of 0.86 $\\pm$ 0.04 across all structures. our method also achieves markedly lower volume errors than the clinical standard, simpson's biplane rule: 4.88 $\\pm$ 4.26 ml vs. 8.14 $\\pm$ 6.04 ml, respectively, for the left ventricle; and 6.40 $\\pm$ 7.37 ml vs. 37.76 $\\pm$ 22.96 ml, respectively, for the left atrium. this suggests that our approach offers a viable route to more accurate 3d chamber quantification in 2d transthoracic echocardiography.\nnone\ncs.cv\ncs.cv cs.ai cs.ce", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05882v1", "title": "EoCD: Encoder only Remote Sensing Change Detection", "abstract": "Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.", "authors": ["Mubashir Noman", "Mustansar Fiaz", "Hiyam Debary", "Abdul Hannan", "Shah Nawaz", "Fahad Shahbaz Khan", "Salman Khan"], "primary_category": "cs.CV", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/2602.05882v1", "entry_url": "http://arxiv.org/abs/2602.05882v1", "published": "2026-02-05T16:58:42+00:00", "updated": "2026-02-05T16:58:42+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:20.287713", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "eocd: encoder only remote sensing change detection\nbeing a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. existing change detection methods rely on the siamese encoder to individually extract temporal features followed by temporal fusion. subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. these aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. alternatively, few methods utilize the early fusion scheme to combine the temporal images. these methods prevent the extra overhead of siamese encoder, however, they also rely on sophisticated decoders for better performance. in addition, these methods demonstrate inferior performance as compared to late fusion based methods. to bridge these gaps, we introduce encoder only change detection (eocd) that is a simple and effective method for the change detection task. the proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. eocd demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. additionally, eocd demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.\nnone\ncs.cv\ncs.cv", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05869v1", "title": "Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity", "abstract": "We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \\times \\cdots \\times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.\n  Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.", "authors": ["Hengrui Luo", "Anna Ma", "Ludovic Stephan", "Yizhe Zhu"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG", "math.NA", "math.PR", "math.ST"], "pdf_url": "https://arxiv.org/pdf/2602.05869v1", "entry_url": "http://arxiv.org/abs/2602.05869v1", "published": "2026-02-05T16:47:13+00:00", "updated": "2026-02-05T16:47:13+00:00", "comment": "58 pages, 3 figures", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.289900", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "wedge sampling: efficient tensor completion with nearly-linear sample complexity\nwe introduce wedge sampling, a new non-adaptive sampling scheme for low-rank tensor completion. we study recovery of an order-$k$ low-rank tensor of dimension $n \\times \\cdots \\times n$ from a subset of its entries. unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. by directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.\n  our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. the approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\\tilde{o}(n)$ uniformly sampled entries, substantially improving over the $\\tilde{o}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. overall, our results suggest that the statistical-to-computational gap highlighted in barak and moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.\n58 pages, 3 figures\nstat.ml\nstat.ml cs.lg math.na math.pr math.st", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05862v1", "title": "Distribution-free two-sample testing with blurred total variation distance", "abstract": "Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.", "authors": ["Rohan Hore", "Rina Foygel Barber"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.05862v1", "entry_url": "http://arxiv.org/abs/2602.05862v1", "published": "2026-02-05T16:43:31+00:00", "updated": "2026-02-05T16:43:31+00:00", "comment": "47 pages, 4 figures", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290000", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "distribution-free two-sample testing with blurred total variation distance\ntwo-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. in particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (tv) distance between the distributions, is impossible to achieve in a distribution-free regime. in this work, we examine the blurred tv distance, a relaxation of tv distance that enables us to perform inference without assumptions on the distributions. we provide theoretical guarantees for distribution-free upper and lower bounds on the blurred tv distance, and examine its properties in high dimensions.\n47 pages, 4 figures\nstat.ml\nstat.ml cs.lg", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05852v1", "title": "Exact Recovery in the Data Block Model", "abstract": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.", "authors": ["Amir R. Asadi", "Akbar Davoodi", "Ramin Javadi", "Farzad Parvaresh"], "primary_category": "cs.LG", "categories": ["cs.LG", "cs.IT", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.05852v1", "entry_url": "http://arxiv.org/abs/2602.05852v1", "published": "2026-02-05T16:36:57+00:00", "updated": "2026-02-05T16:36:57+00:00", "comment": "35 pages", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290210", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "exact recovery in the data block model\ncommunity detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. the stochastic block model (sbm) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. while classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. in this work, we study exact recovery in the data block model (dbm), an sbm augmented with node-associated data, as formalized by asadi, abbe, and verdú (2017). we introduce the chernoff--tv divergence and use it to characterize a sharp exact recovery threshold for the dbm. we further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.\n35 pages\ncs.lg\ncs.lg cs.it stat.ml", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05846v1", "title": "Optimal scaling laws in learning hierarchical multi-index models", "abstract": "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.", "authors": ["Leonardo Defilippis", "Florent Krzakala", "Bruno Loureiro", "Antoine Maillard"], "primary_category": "stat.ML", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.05846v1", "entry_url": "http://arxiv.org/abs/2602.05846v1", "published": "2026-02-05T16:33:51+00:00", "updated": "2026-02-05T16:33:51+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290303", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "optimal scaling laws in learning hierarchical multi-index models\nin this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. we derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. we further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. as a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.\nnone\nstat.ml\nstat.ml cs.lg", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05838v1", "title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation", "abstract": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.", "authors": ["Mayank Kumar", "Qian Lou", "Paulo Barreto", "Martine De Cock", "Sikha Pentyala"], "primary_category": "cs.CR", "categories": ["cs.CR", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.05838v1", "entry_url": "http://arxiv.org/abs/2602.05838v1", "published": "2026-02-05T16:28:13+00:00", "updated": "2026-02-05T16:28:13+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.897408", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "fhaim: fully homomorphic aim for private synthetic data generation\ndata is the lifeblood of ai, yet much of the most valuable data remains locked in silos due to privacy and regulations. as a result, ai remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. synthetic data generation (sdg), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing sdg-as-a-service workflow require data holders to trust providers with access to private data.we propose fhaim, the first fully homomorphic encryption (fhe) framework for training a marginal-based synthetic data generator on encrypted tabular data. fhaim adapts the widely used aim algorithm to the fhe setting using novel fhe protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. our empirical analysis show that fhaim preserves the performance of aim while maintaining feasible runtimes.\nnone\ncs.cr\ncs.cr cs.ai", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05837v1", "title": "Large Data Acquisition and Analytics at Synchrotron Radiation Facilities", "abstract": "Synchrotron facilities like the Cornell High Energy Synchrotron Source (CHESS) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. We present the design, deployment, and evaluation of a framework that addresses CHESS's data acquisition and management issues. Deployed on a secure CHESS server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. Implemented across three beamlines (ID3A, ID3B, ID4B), the framework managed 50-100 TB of data and over 10 million files in late 2024. Testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. Our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.", "authors": ["Aashish Panta", "Giorgio Scorzelli", "Amy A. Gooch", "Werner Sun", "Katherine S. Shanks", "Suchismita Sarker", "Devin Bougie", "Keara Soloway", "Rolf Verberg", "Tracy Berman", "Glenn Tarcea", "John Allison", "Michela Taufer", "Valerio Pascucci"], "primary_category": "cs.HC", "categories": ["cs.HC"], "pdf_url": "https://arxiv.org/pdf/2602.05837v1", "entry_url": "http://arxiv.org/abs/2602.05837v1", "published": "2026-02-05T16:28:06+00:00", "updated": "2026-02-05T16:28:06+00:00", "comment": "10 pages, 11 figures, Accepted at IEEE BigDataW 2025", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:26.746640", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "large data acquisition and analytics at synchrotron radiation facilities\nsynchrotron facilities like the cornell high energy synchrotron source (chess) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. we present the design, deployment, and evaluation of a framework that addresses chess's data acquisition and management issues. deployed on a secure chess server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. implemented across three beamlines (id3a, id3b, id4b), the framework managed 50-100 tb of data and over 10 million files in late 2024. testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.\n10 pages, 11 figures, accepted at ieee bigdataw 2025\ncs.hc\ncs.hc", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05833v1", "title": "Synthesizing Realistic Test Data without Breaking Privacy", "abstract": "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.", "authors": ["Laura Plein", "Alexi Turcotte", "Arina Hallemans", "Andreas Zeller"], "primary_category": "cs.LG", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.05833v1", "entry_url": "http://arxiv.org/abs/2602.05833v1", "published": "2026-02-05T16:22:01+00:00", "updated": "2026-02-05T16:22:01+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290350", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "synthesizing realistic test data without breaking privacy\nthere is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. a lot of research has been done in leveraging generative adversarial networks (gans) for synthetic data generation. however, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (mia) or dataset reconstruction attacks since the original data has been leveraged in the training process. in this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. the approach is inspired by gans, with a generation step and a discrimination step. however, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. by evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. we evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.\nnone\ncs.lg\ncs.lg", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05817v1", "title": "Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows", "abstract": "The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.", "authors": ["Enrique Feito-Casares", "Francisco M. Melgarejo-Meseguer", "Elena Casiraghi", "Giorgio Valentini", "José-Luis Rojo-Álvarez"], "primary_category": "cs.CR", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf_url": "https://arxiv.org/pdf/2602.05817v1", "entry_url": "http://arxiv.org/abs/2602.05817v1", "published": "2026-02-05T16:08:24+00:00", "updated": "2026-02-05T16:08:24+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290447", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "interpreting manifolds and graph neural embeddings from internet of things traffic flows\nthe rapid expansion of internet of things (iot) ecosystems has led to increasingly complex and heterogeneous network topologies. traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. although graph neural networks (gnns) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. this projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. the framework achieves a classification f1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. ultimately, the presented approach bridges the gap between high-dimensional gnn embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.\nnone\ncs.cr\ncs.cr cs.lg cs.ni", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05812v1", "title": "Principled Confidence Estimation for Deep Computed Tomography", "abstract": "We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.", "authors": ["Matteo Gätzner", "Johannes Kirschner"], "primary_category": "cs.LG", "categories": ["cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.05812v1", "entry_url": "http://arxiv.org/abs/2602.05812v1", "published": "2026-02-05T16:04:19+00:00", "updated": "2026-02-05T16:04:19+00:00", "comment": null, "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:33.290543", "meta": {"has_code": false, "has_user_study": false, "is_survey_or_report": false}, "all_text": "principled confidence estimation for deep computed tomography\nwe present a principled framework for confidence estimation in computed tomography (ct) reconstruction. based on the sequential likelihood mixing framework (kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based ct reconstructions. we consider a realistic forward model following the beer-lambert law, i.e., a log-linear forward model with poisson noise, closely reflecting clinical and scientific imaging conditions. the framework is general and applies to both classical algorithms and deep learning reconstruction methods, including u-nets, u-net ensembles, and generative diffusion models. empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. this establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.\nnone\ncs.lg\ncs.lg stat.ml", "recall_hits": [], "recall_hit_count": 0}
{"arxiv_id": "2602.05811v1", "title": "STProtein: predicting spatial protein expression from multi-omics data", "abstract": "The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological \"Dark Matter\".", "authors": ["Zhaorui Jiang", "Yingfang Yuan", "Lei Hu", "Wei Pang"], "primary_category": "cs.AI", "categories": ["cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.05811v1", "entry_url": "http://arxiv.org/abs/2602.05811v1", "published": "2026-02-05T16:04:03+00:00", "updated": "2026-02-05T16:04:03+00:00", "comment": "STProtein: predicting spatial protein expression from multi-omics data is accepted SPARTA_AAAI2026 Oral GitHub: https://github.com/zhaorui-bi/STProtein", "journal_ref": null, "doi": null, "score": 0.0, "matched_keywords": [], "summary": null, "fetched_at": "2026-02-06T17:04:16.897535", "meta": {"has_code": true, "has_user_study": false, "is_survey_or_report": false}, "all_text": "stprotein: predicting spatial protein expression from multi-omics data\nthe integration of spatial multi-omics data from single tissues is crucial for advancing biological research. however, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. to overcome this challenge we propose stprotein, a novel framework leveraging graph neural networks with multi-task learning strategy. stprotein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. we believe that stprotein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. this tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological \"dark matter\".\nstprotein: predicting spatial protein expression from multi-omics data is accepted sparta_aaai2026 oral github: https://github.com/zhaorui-bi/stprotein\ncs.ai\ncs.ai", "recall_hits": [], "recall_hit_count": 0}
