# arXiv Paper Bot - å®Œæ•´æ–‡æ¡£

> **å¿«é€Ÿå¯¼èˆª**
> - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) - 3åˆ†é’Ÿä¸Šæ‰‹
> - [ä½¿ç”¨æµç¨‹](#ä½¿ç”¨æµç¨‹) - æ—¥å¸¸ä½¿ç”¨æŒ‡å—
> - [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„) - æŠ€æœ¯è®¾è®¡
> - [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜) - è¯¦ç»†å‚æ•°
> - [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—) - æ‰©å±•å¼€å‘

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ä½¿ç”¨æµç¨‹](#ä½¿ç”¨æµç¨‹)
3. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
4. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
5. [æŠ€æœ¯å®ç°](#æŠ€æœ¯å®ç°)
6. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
7. [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¸é…ç½®ï¼ˆ2åˆ†é’Ÿï¼‰

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. é…ç½®å…³é”®è¯ï¼ˆç¼–è¾‘ config/config.yamlï¼‰
vim config/config.yaml

# æ ¸å¿ƒé…ç½®ï¼š
#   categories: [cs.CV, cs.AI]     # ä½ å…³æ³¨çš„é¢†åŸŸ
#   search_keywords: [transformer] # å…³é”®è¯
#   fetch_mode: combined           # æ¨èæ¨¡å¼

# 3. è¿è¡Œ
python main.py
```

### ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆ1åˆ†é’Ÿï¼‰

```bash
# æŠ“å–æœ€è¿‘1å¤©çš„è®ºæ–‡
python main.py

# æŸ¥çœ‹ç»“æœ
cat data/papers.json | jq '.[0:3]'

# è¾“å‡ºç¤ºä¾‹ï¼š
# Top 5 Papers:
# 1. Artificial Hippocampus Networks... (Score: 6.0)
#    Keywords: transformer, attention
#    URL: https://arxiv.org/abs/2510.07318
```

---

## ä½¿ç”¨æµç¨‹

### æ—¥å¸¸å·¥ä½œæµï¼ˆæ¯å¤©5åˆ†é’Ÿï¼‰

```
é…ç½® â†’ æŠ“å– â†’ æŸ¥çœ‹ â†’ åé¦ˆ â†’ æ¨é€ â†’ ä¼˜åŒ– â†’ å¾ªç¯
```

#### Step 1: æŠ“å–è®ºæ–‡

```bash
# é»˜è®¤æŠ“å–æœ€è¿‘1å¤©
python main.py

# æŠ“å–æœ€è¿‘7å¤©ï¼ˆå‘¨æŠ¥åœºæ™¯ï¼‰
python main.py --days 7
```

**æ‰§è¡Œè¿‡ç¨‹ï¼š**
```
[1/5] ä» arXiv æŠ“å–è®ºæ–‡
  â”œâ”€ Mode: combined (cs.AI + keywords)
  â”œâ”€ (å¯é€‰) fetch_full_categories=True æ—¶é¢å¤–æ‹‰å–åˆ†ç±»å…¨é‡æ•°æ®
  â””â”€ è¾“å‡º: ~50-100ç¯‡

[2/5] æœ¬åœ°å…³é”®è¯è¿‡æ»¤ä¸è¯„åˆ†
  â”œâ”€ å¤šçº§æƒé‡è®¡ç®—
  â””â”€ è¾“å‡º: ~20ç¯‡

[3/5] AIæ€»ç»“ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰

[4/5] ä¿å­˜ç»“æœ
  â”œâ”€ data/papers.json
  â””â”€ data/papers.csv

[5/5] å¤šæ¸ é“æ¨é€ï¼ˆå¯é€‰ï¼‰
  â””â”€ å°† Top N è®ºæ–‡æ¨é€åˆ°é…ç½®çš„æ¸ é“
```

#### Step 2: æŸ¥çœ‹ç»“æœ

```bash
# JSONæ ¼å¼ï¼ˆç¨‹åºåŒ–å¤„ç†ï¼‰
cat data/papers.json

# åªçœ‹é«˜åˆ†è®ºæ–‡ï¼ˆåˆ†æ•°>=5ï¼‰
cat data/papers.json | jq '.[] | select(.score >= 5)'

# CSVæ ¼å¼ï¼ˆExcelæ‰“å¼€ï¼‰
open data/papers.csv
```

#### Step 3: åé¦ˆæ”¶é›†ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰â­

```bash
# ğŸ‘ å–œæ¬¢è¿™ç¯‡è®ºæ–‡
python feedback.py like 2510.07318

# ğŸ‘ ä¸å–œæ¬¢
python feedback.py dislike 2501.99999

# ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡
python feedback.py stats

# è¾“å‡ºï¼š
# ğŸ“Š User Feedback Statistics
#   Total liked: 15 papers
#   Top Keywords in Liked Papers:
#   - transformer: 12 papers (80%)
#   - multimodal: 8 papers (53%)
```

```bash
# æŸ¥çœ‹å·²ç‚¹èµçš„è®ºæ–‡åˆ—è¡¨
python feedback.py list liked

# è¯¦ç»†æ¨¡å¼ï¼ˆæ˜¾ç¤ºå…³é”®è¯ã€æ—¶é—´ï¼‰
python feedback.py list liked --verbose

# æ¸…ç©ºåé¦ˆæ•°æ®
python feedback.py clear all
```

#### Step 4: å¤šæ¸ é“æ¨é€ï¼ˆå¯é€‰ï¼‰

```bash
# å¯ç”¨æ¨é€å‰è¯·åœ¨ config/config.yaml å¡«å†™ notification é…ç½®
python main.py  # è¿è¡Œåè‡ªåŠ¨æ¨é€å½“æ—¥ Top N è®ºæ–‡
```

**æ”¯æŒæ¸ é“ï¼š**

- é£ä¹¦ç¾¤æœºå™¨äººï¼šé…ç½® `notification.feishu.webhook_url`ï¼ˆå¦‚å¼€å¯ç­¾ååˆ™å¡«å†™ `secret`ï¼‰ã€‚
- Telegram Botï¼šé…ç½® `notification.telegram.bot_token` ä¸ `chat_id`ã€‚
- å¾®ä¿¡å…¬ä¼—å·ï¼šé…ç½® `notification.wechat`ï¼ˆéœ€è¦å·²å…³æ³¨å…¬ä¼—å·çš„ç”¨æˆ· OpenIDï¼‰ã€‚

ç¨‹åºä¼šåœ¨ä¿å­˜ `papers.json` / `papers.csv` åè°ƒç”¨æ¨é€æ¨¡å—ï¼Œå¹¶å°†å…³é”®ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å¾—åˆ†ã€å…³é”®è¯ã€é“¾æ¥ï¼‰ä»¥æ–‡æœ¬å½¢å¼å‘é€ã€‚

#### Step 5: ä¼˜åŒ–é…ç½®ï¼ˆæ¯å‘¨ä¸€æ¬¡ï¼‰

```bash
# 1. æŸ¥çœ‹åé¦ˆç»Ÿè®¡
python feedback.py stats

# 2. æ ¹æ®ç»Ÿè®¡ç»“æœè°ƒæ•´ config.yaml
vim config/config.yaml

# ä¾‹å¦‚ï¼šå¦‚æœ"transformer"å‡ºç°åœ¨80%çš„likedè®ºæ–‡ä¸­
# â†’ å°†å…¶æå‡åˆ° high_priority
```

### å…¸å‹åœºæ™¯

#### åœºæ™¯1ï¼šæ¯å¤©æ—©ä¸Šè·å–æ¨è

```bash
# Linux/Mac cronå®šæ—¶
crontab -e
# æ·»åŠ ï¼š0 9 * * * cd /path/to/arxiv_paper && python main.py
```

#### åœºæ™¯2ï¼šå¿«é€Ÿæµ‹è¯•æ–°å…³é”®è¯

```bash
# 1. ä¿®æ”¹ config.yaml æ·»åŠ æ–°å…³é”®è¯
# 2. æµ‹è¯•æ¨¡å¼
python main.py --test

# 3. æ£€æŸ¥ç»“æœ
cat data/papers.json | jq '.[0:3]'

# 4. æ»¡æ„åæ­£å¼è¿è¡Œ
python main.py
```

#### åœºæ™¯3ï¼šå‘¨æŠ¥æ•´ç†

```bash
python main.py --days 7
open data/papers.csv
```

---

## ç³»ç»Ÿæ¶æ„

### æ¼æ–—ç­›é€‰æ¶æ„

ç³»ç»Ÿé‡‡ç”¨**å¤šçº§æ¼æ–—æ¶æ„**ï¼Œä»500+ç¯‡è®ºæ–‡ç­›é€‰åˆ°5-10ç¯‡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  500+ Daily Papers on arXiv         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Level 1: API-Level Filtering       â”‚
â”‚  â€¢ combined: (keywords) AND cat     â”‚
â”‚  â€¢ Reduce 90% papers                â”‚
â”‚  Output: ~50-100 papers             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Level 2: Local Keyword Scoring     â”‚
â”‚  â€¢ Multi-tier weighting             â”‚
â”‚  â€¢ Top-K selection                  â”‚
â”‚  Output: ~20 papers                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Slot 1: Vector Similarity ğŸ”²       â”‚
â”‚  â€¢ SPECTER embeddings               â”‚
â”‚  â€¢ Status: Reserved (Phase 1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Slot 2: Agent Intent ğŸ”²            â”‚
â”‚  â€¢ LLM pattern analysis             â”‚
â”‚  â€¢ Status: Reserved (Phase 2)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final List (5-10 papers)           â”‚
â”‚  â€¢ User Feedback Loop               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµ

#### è¾“å…¥ï¼ˆarXiv APIï¼‰
```json
{
  "arxiv_id": "2510.07318",
  "title": "...",
  "abstract": "...",
  "categories": ["cs.AI"]
}
```

#### è¾“å‡ºï¼ˆdata/papers.jsonï¼‰
```json
{
  "arxiv_id": "2510.07318",
  "title": "...",
  "score": 6.0,                    // âœ… å…³é”®è¯è¯„åˆ†
  "matched_keywords": [            // âœ… åŒ¹é…çš„å…³é”®è¯
    "transformer",
    "attention"
  ],

  // ğŸ”² é¢„ç•™å­—æ®µï¼ˆPhase 1/2ï¼‰
  "similarity_score": null,
  "personalized_score": null,
  "user_feedback": null
}
```

#### åé¦ˆæ•°æ®ï¼ˆdata/feedback/ï¼‰
```
liked_papers.json         // ç‚¹èµåˆ—è¡¨
disliked_papers.json      // ä¸å–œæ¬¢åˆ—è¡¨
user_profile.json         // ç”¨æˆ·ç”»åƒï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
```

### 4ç§æŠ“å–æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | APIæŸ¥è¯¢ | æ•ˆç‡ | é€‚ç”¨åœºæ™¯ |
|------|---------|------|---------|
| `category_only` | `cat:cs.AI` | â­â­â­ | åªå…³å¿ƒé¢†åŸŸ |
| `keyword_only` | `"transformer"` | â­â­â­ | è·¨é¢†åŸŸæœç´¢ |
| **`combined`** â­ | `("transformer") AND cat:cs.AI` | â­â­â­â­â­ | **æ¨è** |
| `category_then_filter` | `cat:cs.AI` | â­â­ | å…¼å®¹æ¨¡å¼ |

**æ¨è**ï¼šä½¿ç”¨ `combined` æ¨¡å¼ï¼Œåœ¨APIçº§åˆ«å°±è¿‡æ»¤90%æ— å…³è®ºæ–‡ï¼

### å®ç°é˜¶æ®µ

```
âœ… Phase 0 (å·²å®Œæˆ)
  â€¢ arXiv APIæŠ“å–ï¼ˆ4ç§æ¨¡å¼ï¼‰
  â€¢ å…³é”®è¯è¿‡æ»¤ä¸è¯„åˆ†
  â€¢ JSON/CSVå­˜å‚¨

âœ… Phase 0.5 (å·²å®Œæˆ) â­
  â€¢ FeedbackCollectoræ¨¡å—
  â€¢ feedback.py CLIå·¥å…·
  â€¢ ç”¨æˆ·ç”»åƒç”Ÿæˆ

ğŸ”² Phase 1 (é¢„ç•™ï¼Œ1-2å¤©å·¥ä½œé‡)
  â€¢ SPECTERè®ºæ–‡embedding
  â€¢ ChromaDBå‘é‡æ•°æ®åº“
  â€¢ ç›¸ä¼¼åº¦æ’åº

ğŸ”² Phase 2 (é¢„ç•™ï¼Œ3-5å¤©å·¥ä½œé‡)
  â€¢ LLMé˜…è¯»æ¨¡å¼åˆ†æ
  â€¢ åŠ¨æ€å…³é”®è¯ç”Ÿæˆ
  â€¢ æ¨èç†ç”±è§£é‡Š
```

---

## é…ç½®è¯´æ˜

### æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼šconfig/config.yaml

#### 1. arXivæŠ“å–è®¾ç½®

```yaml
arxiv:
  # å…³æ³¨çš„é¢†åŸŸ
  categories:
    - cs.CV    # è®¡ç®—æœºè§†è§‰
    - cs.AI    # äººå·¥æ™ºèƒ½
    - cs.LG    # æœºå™¨å­¦ä¹ 

  # æ¯ä¸ªç±»åˆ«æœ€å¤šæŠ“å–æ•°é‡
  max_results: 50

  # æŠ“å–æ¨¡å¼ï¼ˆæ¨è combinedï¼‰
  fetch_mode: combined

  # å½“ä½¿ç”¨å…³é”®è¯æŠ“å–æ—¶æ˜¯å¦é¢å¤–æ‹‰å–å®Œæ•´åˆ†ç±»ä¾›æœ¬åœ°è¿‡æ»¤
  fetch_full_categories: false

  # APIçº§å…³é”®è¯ï¼ˆç”¨äºcombinedæ¨¡å¼ï¼‰
  search_keywords:
    - transformer
    - diffusion
    - multimodal
```

**å¸¸ç”¨arXivåˆ†ç±»ï¼š**
- `cs.CV` - è®¡ç®—æœºè§†è§‰
- `cs.AI` - äººå·¥æ™ºèƒ½
- `cs.LG` - æœºå™¨å­¦ä¹ 
- `cs.CL` - NLP
- `cs.RO` - æœºå™¨äºº
- `stat.ML` - ç»Ÿè®¡æœºå™¨å­¦ä¹ 

#### 2. è¿‡æ»¤è®¾ç½®

```yaml
filter:
  enabled: true

  # å…³é”®è¯åŠæƒé‡
  keywords:
    high_priority:        # æƒé‡ 3.0
      - transformer
      - diffusion
      - multimodal

    medium_priority:      # æƒé‡ 2.0
      - detection
      - segmentation

    low_priority:         # æƒé‡ 1.0
      - deep learning

  # æœ€ä½åˆ†æ•°é˜ˆå€¼
  min_score: 1.0

  # ä¿ç•™å‰Kç¯‡
  top_k: 20
```

**è¯„åˆ†æœºåˆ¶ï¼š**
```python
score = Î£(keyword_weight Ã— match_count)
```

#### 3. ä¸ªæ€§åŒ–è®¾ç½®ï¼ˆé¢„ç•™ï¼‰

```yaml
personalization:
  enabled: false         # ğŸ”² æš‚æœªå®ç°

  feedback:
    enabled: true        # âœ… åé¦ˆæ”¶é›†å¯ç”¨
    feedback_dir: data/feedback

  vector_ranking:        # ğŸ”² Phase 1
    enabled: false
    model: allenai/specter
    weight: 0.4

  agent:                 # ğŸ”² Phase 2
    enabled: false
    provider: openai
    model: gpt-4o-mini
```

#### 4. å­˜å‚¨è®¾ç½®

```yaml
storage:
  format: both           # json/csv/both
  json_path: data/papers.json
  csv_path: data/papers.csv
  append_mode: true      # è¿½åŠ æ¨¡å¼ï¼Œè‡ªåŠ¨å»é‡
```

#### 5. AIæ€»ç»“è®¾ç½®ï¼ˆå¯é€‰ï¼‰

```yaml
summarization:
  enabled: false         # é»˜è®¤å…³é—­
  provider: gemini
  api:
    base_url: "https://api.example.com/v1"
    model: "gemini-2.5-flash"
    api_key_env: "ARXIV_API_KEY"
  fields:
    - one_sentence_highlight
    - core_method
```

#### 6. é€šçŸ¥æ¨é€è®¾ç½®ï¼ˆå¯é€‰ï¼‰

```yaml
notification:
  enabled: true              # æ˜¯å¦å¼€å¯æ¨é€
  provider: feishu           # feishu / telegram / wechat
  top_k: 5                   # æ¨é€æ¡æ•°

  feishu:
    webhook_url: https://open.feishu.cn/xxx
    secret: your-secret-if-enabled

  telegram:
    bot_token: 123456:ABCDEF
    chat_id: "-100123456"

  wechat:
    app_id: wx1234567890
    app_secret: your-app-secret
    open_id: user-open-id
```

**æ³¨æ„äº‹é¡¹ï¼š**
- é£ä¹¦ï¼šéœ€æå‰åœ¨ç›®æ ‡ç¾¤ç»„åˆ›å»ºè‡ªå®šä¹‰æœºå™¨äººï¼›è‹¥å¯ç”¨ç­¾åæ ¡éªŒï¼Œè¯·å¡«å†™ `secret`ã€‚
- Telegramï¼šè·å– `chat_id` å¯é€šè¿‡ `@userinfobot` æˆ–è°ƒç”¨ API `getUpdates`ã€‚
- å¾®ä¿¡å…¬ä¼—å·ï¼šä»…æ”¯æŒå·²å…³æ³¨å…¬ä¼—å·çš„ç”¨æˆ·ï¼Œéœ€å…·å¤‡å®¢æœæ¶ˆæ¯æƒé™ã€‚

---

## æŠ€æœ¯å®ç°

### é¡¹ç›®ç»“æ„

```
arxiv_paper/
â”œâ”€â”€ main.py                      # âœ… ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ feedback.py                  # âœ… åé¦ˆç®¡ç†CLI
â”œâ”€â”€ requirements.txt             # âœ… ä¾èµ–åˆ—è¡¨
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # âœ… é…ç½®æ–‡ä»¶
â”‚
â”œâ”€â”€ src/                         # âœ… æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fetcher.py               # arXivæŠ“å–
â”‚   â”œâ”€â”€ filter.py                # å…³é”®è¯è¿‡æ»¤
â”‚   â”œâ”€â”€ storage.py               # å­˜å‚¨
â”‚   â”œâ”€â”€ models.py                # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ summarizer.py            # AIæ€»ç»“
â”‚   â”œâ”€â”€ notifier.py              # å¤šæ¸ é“é€šçŸ¥
â”‚   â”œâ”€â”€ feedback.py              # åé¦ˆæ”¶é›† â­
â”‚   â””â”€â”€ personalization.py       # ä¸ªæ€§åŒ–ï¼ˆé¢„ç•™ï¼‰ğŸ”²
â”‚
â”œâ”€â”€ data/                        # âœ… æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ papers.json
â”‚   â”œâ”€â”€ papers.csv
â”‚   â””â”€â”€ feedback/                # åé¦ˆæ•°æ® â­
â”‚       â”œâ”€â”€ liked_papers.json
â”‚       â”œâ”€â”€ disliked_papers.json
â”‚       â””â”€â”€ user_profile.json
â”‚
â”œâ”€â”€ tests/                       # âœ… å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_filter.py
â”‚   â”œâ”€â”€ test_storage.py          # å­˜å‚¨è¯»å†™
â”‚   â””â”€â”€ test_notifier.py         # æ¨é€é€»è¾‘ï¼ˆHTTP è°ƒç”¨mockï¼‰
â”‚
â””â”€â”€ logs/
    â””â”€â”€ arxiv_bot.log
```

### æ ¸å¿ƒæ¨¡å—

#### 1. fetcher.py - è®ºæ–‡æŠ“å–

```python
from src import ArxivFetcher, FetchConfig

config = FetchConfig(
    categories=["cs.AI"],
    max_results=50,
    fetch_mode="combined",
    search_keywords=["transformer"]
)

fetcher = ArxivFetcher(config)
papers = fetcher.fetch_latest_papers(days=1)
```

**ç‰¹æ€§ï¼š**
- âœ… 4ç§æŠ“å–æ¨¡å¼
- âœ… è‡ªåŠ¨å»é‡
- âœ… æ—¥æœŸè¿‡æ»¤
- âœ… å®Œæ•´å…ƒæ•°æ®

#### 2. filter.py - å…³é”®è¯è¿‡æ»¤

```python
from src import PaperFilter, FilterConfig

config = FilterConfig(
    enabled=True,
    keywords={
        "high_priority": ["transformer"],
        "medium_priority": ["detection"]
    },
    min_score=1.0,
    top_k=20
)

filter = PaperFilter(config)
ranked = filter.filter_and_rank(papers)
```

**è¯„åˆ†ç®—æ³•ï¼š**
- åœ¨æ ‡é¢˜+æ‘˜è¦ä¸­åŒ¹é…å…³é”®è¯
- ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ `\bå…³é”®è¯\b`
- ç´¯åŠ æƒé‡åˆ†æ•°

#### 3. feedback.py - åé¦ˆæ”¶é›†

```python
from src import FeedbackCollector

collector = FeedbackCollector()

# è®°å½•åé¦ˆ
collector.record_feedback("2510.07318", "like", paper_data)

# è·å–ç»Ÿè®¡
stats = collector.get_statistics()
# {
#   "total_liked": 15,
#   "total_disliked": 3,
#   "top_keywords": [("transformer", 12), ...],
#   "feedback_ratio": 0.833
# }

# è·å–ç”¨æˆ·åå¥½å…³é”®è¯
keywords = collector.get_user_keywords()
# {"transformer": 12, "multimodal": 8, ...}
```

#### 4. storage.py - æ•°æ®å­˜å‚¨

```python
from src import PaperStorage

storage = PaperStorage(
    json_path="data/papers.json",
    csv_path="data/papers.csv",
    append_mode=True
)

# ä¿å­˜ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
storage.save(papers, format="both")
```

### å…³é”®ç®—æ³•

#### å…³é”®è¯åŒ¹é…
```python
def _score_paper(paper: Paper) -> tuple[float, List[str]]:
    text = f"{paper.title} {paper.abstract}".lower()
    score = 0.0
    matched = []

    for keyword, weight in keywords.items():
        pattern = r"\b" + re.escape(keyword) + r"\b"
        if re.search(pattern, text):
            score += weight
            matched.append(keyword)

    return score, matched
```

#### å»é‡é€»è¾‘
```python
def _deduplicate_papers(papers: List[Paper]) -> List[Paper]:
    seen_ids = set()
    unique = []

    for paper in papers:
        if paper.arxiv_id not in seen_ids:
            seen_ids.add(paper.arxiv_id)
            unique.append(paper)

    return unique
```

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šæ— æ³•æŠ“å–è®ºæ–‡ï¼ˆFetched 0 papersï¼‰

**å¯èƒ½åŸå› ï¼š**
1. dayså‚æ•°å¤ªå°ï¼Œæœ€è¿‘æ²¡æœ‰æ–°è®ºæ–‡
2. å…³é”®è¯å¤ªä¸¥æ ¼
3. ç½‘ç»œé—®é¢˜

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# å¢åŠ å¤©æ•°
python main.py --days 7

# é™ä½min_score
# config.yaml: min_score: 0.5

# æµ‹è¯•ç½‘ç»œ
curl https://export.arxiv.org/api/query
```

### é—®é¢˜2ï¼šåˆ†æ•°éƒ½å¾ˆä½

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# 1. å¢åŠ å…³é”®è¯
keywords:
  low_priority:
    - deep learning
    - neural network
    - machine learning

# 2. é™ä½é˜ˆå€¼
min_score: 0.5

# 3. ä½¿ç”¨combinedæ¨¡å¼
fetch_mode: combined
```

### é—®é¢˜3ï¼šåé¦ˆç»Ÿè®¡ä¸ºç©º

**æ£€æŸ¥ï¼š**
```bash
# æŸ¥çœ‹åé¦ˆæ–‡ä»¶
ls -la data/feedback/

# æŸ¥çœ‹likedè®ºæ–‡
cat data/feedback/liked_papers.json

# é‡æ–°è®°å½•åé¦ˆ
python feedback.py like <paper_id>
```

### é—®é¢˜4ï¼šTimezoneé”™è¯¯

**å·²ä¿®å¤**ï¼ˆsrc/fetcher.pyï¼‰ï¼š
```python
# ä¿®æ”¹å‰ï¼š
cutoff_date = datetime.now() - timedelta(days=days)

# ä¿®æ”¹åï¼š
cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
```

---

## å¼€å‘æŒ‡å—

### æ‰©å±•Phase 1ï¼šå‘é‡ç›¸ä¼¼åº¦

**æ­¥éª¤ï¼š**

1. **å®‰è£…ä¾èµ–**
```bash
# å–æ¶ˆæ³¨é‡Š requirements.txt
pip install sentence-transformers chromadb
```

2. **å¯ç”¨é…ç½®**
```yaml
# config.yaml
personalization:
  vector_ranking:
    enabled: true
    model: allenai/specter
```

3. **å®ç°æ–¹æ³•**ï¼ˆsrc/personalization.pyï¼‰
```python
def compute_embedding(self, paper: Paper) -> np.ndarray:
    """å®ç°è®ºæ–‡embeddingè®¡ç®—"""
    text = f"{paper.title} {paper.abstract}"
    return self.model.encode(text)

def rank_by_similarity(
    self,
    papers: List[Paper],
    liked_papers: List[Paper]
) -> List[Paper]:
    """å®ç°ç›¸ä¼¼åº¦æ’åº"""
    # 1. è®¡ç®—æ‰€æœ‰è®ºæ–‡çš„embedding
    # 2. è®¡ç®—likedè®ºæ–‡çš„å¹³å‡embedding
    # 3. è®¡ç®—cosineç›¸ä¼¼åº¦
    # 4. ç»“åˆkeyword scoreé‡æ’åº
    pass
```

### æ‰©å±•Phase 2ï¼šAgentæ„å›¾è¯†åˆ«

**æ­¥éª¤ï¼š**

1. **é…ç½®LLM**
```yaml
personalization:
  agent:
    enabled: true
    provider: openai
    model: gpt-4o-mini
```

2. **å®ç°æ–¹æ³•**ï¼ˆsrc/personalization.pyï¼‰
```python
def analyze_reading_pattern(
    self,
    liked_papers: List[Paper]
) -> dict:
    """å®ç°é˜…è¯»æ¨¡å¼åˆ†æ"""
    # 1. æå–likedè®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦
    # 2. è°ƒç”¨LLMåˆ†æ
    # 3. è¿”å›ç»“æ„åŒ–ç»“æœ
    return {
        "main_interests": ["multimodal", "transformer"],
        "suggested_keywords": ["CLIP", "vision-language"]
    }
```

### æ·»åŠ æ–°çš„æ•°æ®æº

**ç¤ºä¾‹ï¼šä»ä¼šè®®ç½‘ç«™æŠ“å–**
```python
# src/conference_fetcher.py
class ConferenceFetcher:
    def fetch_papers(self, conference: str) -> List[Paper]:
        # å®ç°ä»ä¼šè®®ç½‘ç«™æŠ“å–
        pass

# main.py
arxiv_papers = arxiv_fetcher.fetch()
conf_papers = conference_fetcher.fetch("CVPR")
all_papers = arxiv_papers + conf_papers
```

### å•å…ƒæµ‹è¯•

**è¿è¡Œæµ‹è¯•ï¼š**
```bash
python run_tests.py

# æˆ–ä½¿ç”¨pytest
pytest tests/ -v
```

**æ·»åŠ æ–°æµ‹è¯•ï¼š**
```python
# tests/test_feedback.py
import unittest
from src import FeedbackCollector

class TestFeedback(unittest.TestCase):
    def test_record_feedback(self):
        collector = FeedbackCollector()
        collector.record_feedback("2510.07318", "like")
        liked = collector.get_liked_papers()
        self.assertEqual(len(liked), 1)
```

---

## é™„å½•

### A. å®Œæ•´å‘½ä»¤å‚è€ƒ

```bash
# ä¸»ç¨‹åº
python main.py                    # æŠ“å–æœ€è¿‘1å¤©
python main.py --days 7           # æŠ“å–æœ€è¿‘7å¤©
python main.py --test             # æµ‹è¯•æ¨¡å¼
python main.py --config custom.yaml  # è‡ªå®šä¹‰é…ç½®

# åé¦ˆç®¡ç†
python feedback.py like <id>      # ç‚¹èµ
python feedback.py dislike <id>   # ä¸å–œæ¬¢
python feedback.py stats          # ç»Ÿè®¡
python feedback.py list liked     # åˆ—è¡¨
python feedback.py list liked -v  # è¯¦ç»†åˆ—è¡¨
python feedback.py clear all      # æ¸…ç©º

# æµ‹è¯•
python run_tests.py               # è¿è¡Œæ‰€æœ‰æµ‹è¯•
```

### B. æ–‡ä»¶æ ¼å¼ç¤ºä¾‹

**papers.json å®Œæ•´æ ¼å¼ï¼š**
```json
[
  {
    "arxiv_id": "2510.07318v1",
    "title": "Artificial Hippocampus Networks...",
    "abstract": "Full abstract text...",
    "authors": ["Author 1", "Author 2"],
    "primary_category": "cs.AI",
    "categories": ["cs.AI", "cs.LG"],
    "pdf_url": "https://arxiv.org/pdf/2510.07318",
    "entry_url": "https://arxiv.org/abs/2510.07318",
    "published": "2025-10-05T12:00:00Z",
    "updated": "2025-10-05T12:00:00Z",
    "score": 6.0,
    "matched_keywords": ["transformer", "attention"],
    "summary": null,
    "fetched_at": "2025-10-09T10:30:00Z"
  }
]
```

**user_profile.json æ ¼å¼ï¼š**
```json
{
  "updated_at": "2025-10-09T12:00:00",
  "statistics": {
    "total_liked": 15,
    "total_disliked": 3,
    "feedback_ratio": 0.833
  },
  "preferred_keywords": {
    "transformer": 12,
    "multimodal": 8,
    "vision-language": 6
  }
}
```

### C. æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å‡å°‘APIè°ƒç”¨**
   - ä½¿ç”¨ `combined` æ¨¡å¼
   - åˆç†è®¾ç½® `max_results`

2. **æœ¬åœ°ç¼“å­˜**
   - ä½¿ç”¨ `append_mode: true`
   - å®šæœŸæ¸…ç†æ—§æ•°æ®

3. **å¹¶è¡Œå¤„ç†**ï¼ˆæœªæ¥ï¼‰
   - Phase 1: æ‰¹é‡embeddingè®¡ç®—
   - ä½¿ç”¨GPUåŠ é€Ÿ

### D. ç›¸å…³èµ„æº

- [arXiv APIæ–‡æ¡£](https://arxiv.org/help/api)
- [arXiv Pythonåº“](https://github.com/lukasschwab/arxiv.py)
- [arXivåˆ†ç±»åˆ—è¡¨](https://arxiv.org/category_taxonomy)
- [SPECTERæ¨¡å‹](https://github.com/allenai/specter)

---

## ç‰ˆæœ¬å†å²

**v0.1.0** (2025-10-09)
- âœ… Phase 0: åŸºç¡€åŠŸèƒ½ï¼ˆæŠ“å–ã€è¿‡æ»¤ã€å­˜å‚¨ï¼‰
- âœ… Phase 0.5: åé¦ˆç³»ç»Ÿ
- ğŸ”² Phase 1: å‘é‡ç›¸ä¼¼åº¦ï¼ˆé¢„ç•™ï¼‰
- ğŸ”² Phase 2: Agentæ„å›¾ï¼ˆé¢„ç•™ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v0.1.0
**æœ€åæ›´æ–°**: 2025-10-09
**çŠ¶æ€**: âœ… ç”Ÿäº§å¯ç”¨ | ğŸ”² é¢„ç•™æ§½ä½å·²å°±ç»ª
